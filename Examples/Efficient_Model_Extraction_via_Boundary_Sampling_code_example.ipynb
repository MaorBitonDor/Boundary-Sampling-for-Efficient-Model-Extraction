{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zS2X0aH1v5TM"
      },
      "source": [
        "# Efficient Model Extraction via Boundary Sampling\n",
        "\n",
        "*In this notebook we provide code for running our BAM model extraction attack algorithm.*\n",
        "\n",
        "❌ **Warning**: You will not be able to extract the complete dataset $\\mathcal{D}_1$ (30 iteration instead of 40) and you will not be able to train $f'$ for enough epochs. This is because of the GPU-timeout limitation of a free colab instances. Therefore, this code has been modified to train $f'$ with fewer epochs (20 instead of the 40 as used in the paper) and fewer iterations (30 instead of the 40 as used in the paper). Please feel free to copy this notebook and experiment with the hyperparameters on dedicated hardware.\n",
        "\n",
        "## Scenario\n",
        "- Victim model $f$: AlexNet\n",
        "- Training set $\\mathcal{D}_0$: CIFAR-10\n",
        "\n",
        "- Substitute model $f'$ ResNet18\n",
        "- Population size $N$: 20,000\n",
        "- Selection size $k$: 6,000\n",
        "- Iteration count $I$: 30 (instead of 40)\n",
        "- Training epochs: 20 (instead of 40)\n",
        "\n",
        "# Evaluation\n",
        "- Accuracy of the victim and stolen models\n",
        "- Attack Success Rate for transferred adversarail examples\n",
        "\n",
        "We plan to release a complete GitHub including documentation with our paper's camera-ready publication."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibeR1CcjViYZ"
      },
      "source": [
        "# 1. Install Dependencies and Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_cJK_pykadZ",
        "outputId": "d7dc3f5c-28dd-4f80-f9e9-e60d1fd855dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting adversarial-robustness-toolbox\n",
            "  Downloading adversarial_robustness_toolbox-1.17.1-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from adversarial-robustness-toolbox) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from adversarial-robustness-toolbox) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn>=0.22.2 in /usr/local/lib/python3.10/dist-packages (from adversarial-robustness-toolbox) (1.2.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from adversarial-robustness-toolbox) (1.16.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from adversarial-robustness-toolbox) (67.7.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from adversarial-robustness-toolbox) (4.66.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.2->adversarial-robustness-toolbox) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.2->adversarial-robustness-toolbox) (3.3.0)\n",
            "Installing collected packages: adversarial-robustness-toolbox\n",
            "Successfully installed adversarial-robustness-toolbox-1.17.1\n"
          ]
        }
      ],
      "source": [
        "!pip install adversarial-robustness-toolbox"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTPkvnOWwNxB"
      },
      "source": [
        "Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jhWmk54agzpQ"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import gc\n",
        "import logging\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import sys\n",
        "import time\n",
        "import requests\n",
        "from statistics import mean\n",
        "from torch import optim\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.parallel\n",
        "from art.attacks.evasion import ProjectedGradientDescentPyTorch\n",
        "from art.estimators.classification import PyTorchClassifier\n",
        "from matplotlib import pyplot as plt\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
        "from torchvision import datasets, transforms\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArFAzt6LV1C5"
      },
      "source": [
        "# 2. Define Models and Helper Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHseOcgB1c6W"
      },
      "source": [
        "Function to Randomly Generate the Initial Generation for the Evolutionary Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fo2EazhQrIHZ"
      },
      "outputs": [],
      "source": [
        "def generate_random_data_cifar_old(num_images):\n",
        "    \"\"\"\n",
        "    Generate `num_images` random input data in the shape of (num_images, 3, 32, 32)\n",
        "    \"\"\"\n",
        "    image_size = (1, 3, 32, 32)\n",
        "    data = [torch.rand(image_size).cpu() for _ in range(num_images)]\n",
        "    return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahTP-GYJ1pt4"
      },
      "source": [
        "Function to Download the Victim Model from Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4aq2aQsZpjlg"
      },
      "outputs": [],
      "source": [
        "def download_file(url, filename):\n",
        "    \"\"\"Download a file from a given URL\"\"\"\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()  # Check if the request was successful\n",
        "\n",
        "    with open(filename, 'wb') as f:\n",
        "        f.write(response.content)\n",
        "\n",
        "# Example usage\n",
        "url = 'https://drive.google.com/uc?id=10MJ2lSvolpqzY4n5z3lK_HKtC7T068t7'\n",
        "\n",
        "filename = 'teacher_alexnet_for_cifar10_state_dict'\n",
        "download_file(url, filename)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLb-QdWTy9Mp"
      },
      "source": [
        "Function that saves the generated data to the disk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FH_F1hHRyBHy"
      },
      "outputs": [],
      "source": [
        "def save_tensor_list_to_file(tensor_list, file_path):\n",
        "    tensor_array_input = [x[0].detach().cpu().numpy() for x in tensor_list]\n",
        "    tensor_array_labels = [x[1].detach().cpu().numpy() if type(x[1]) is not np.int64 else np.array(x[1]) for x\n",
        "                           in tensor_list]\n",
        "\n",
        "    np.save(file_path + f\"_input\", tensor_array_input)\n",
        "    np.save(file_path + f\"_labels\", tensor_array_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d16JxASAy4ym"
      },
      "source": [
        "Configuration instance creator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XI727CeoiwHb"
      },
      "outputs": [],
      "source": [
        "def create_config(log_file=None):\n",
        "    if log_file is None:\n",
        "        Config()\n",
        "    else:\n",
        "        Config(log_file=log_file)\n",
        "\n",
        "def prepare_config():\n",
        "    create_config()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIYYTBW7zvhe"
      },
      "source": [
        "Function to Evaluate the Attack Success Rate of the Surrogate Model Through Testing the Transferability of Successful Adversarial Samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRodwUSNh6Gh"
      },
      "outputs": [],
      "source": [
        "def get_new_data_loader(model, data_loader, device):\n",
        "    correct_images = []\n",
        "    correct_labels = []\n",
        "    input_shape = None\n",
        "    for inputs, labels in data_loader:\n",
        "        # Assuming inputs is a batch of images\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        if input_shape is None:\n",
        "            input_shape = tuple(inputs.shape[1:])\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        correct_mask = predicted == labels\n",
        "        correct_images.append(inputs[correct_mask])\n",
        "        correct_labels.append(labels[correct_mask])\n",
        "\n",
        "    correct_images = torch.cat(correct_images, dim=0)\n",
        "    correct_labels = torch.cat(correct_labels, dim=0)\n",
        "\n",
        "    correct_dataset = TensorDataset(correct_images, correct_labels)\n",
        "    correct_data_loader = DataLoader(correct_dataset, batch_size=data_loader.batch_size, shuffle=False)\n",
        "    return correct_data_loader, input_shape\n",
        "\n",
        "\n",
        "def test_trans(surrogate_model, victim, loss, num_classes, data_loader):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    new_data_loader, input_shape = get_new_data_loader(victim, data_loader, device)\n",
        "    equality_tensor_p = 0\n",
        "    loss = nn.CrossEntropyLoss()\n",
        "    for batch, (images, labels) in enumerate(new_data_loader):\n",
        "        clf = PyTorchClassifier(model=surrogate_model, loss=loss,\n",
        "                                input_shape=input_shape, nb_classes=num_classes)\n",
        "\n",
        "        PGD_attack = ProjectedGradientDescentPyTorch(estimator=clf, max_iter=random.randint(10, 20), eps=30 / 255,\n",
        "                                                     num_random_init=1)\n",
        "\n",
        "        victim = victim.to(device)\n",
        "        x_n = images.cpu().numpy()\n",
        "\n",
        "        # random pertubations\n",
        "        x_p = PGD_attack.generate(x=x_n)\n",
        "        x_p = torch.from_numpy(x_p)\n",
        "        x_p = x_p.to(device)\n",
        "        adv_label_p = victim(x_p).argmax(dim=1)\n",
        "        # adv_label_p = adv_label_p.to('cpu')\n",
        "\n",
        "        equality_tensor_p += torch.sum((labels != adv_label_p).int()).item()\n",
        "    attack_success_rate = equality_tensor_p / len(new_data_loader.sampler)\n",
        "    return attack_success_rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MViKetc0vDm"
      },
      "source": [
        "Functions for Data and Model Preparation Prior to Training the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2nyBdXGwjxOQ"
      },
      "outputs": [],
      "source": [
        "def prepare_for_training(self_model, model_name, optimizer):\n",
        "    start_epoch = 0\n",
        "    directory = f\"checkpoints/{self_model.__class__.__name__}\"\n",
        "    if Config.instance[\"delete_checkpoints\"]:\n",
        "        import shutil\n",
        "        try:\n",
        "            shutil.rmtree(directory)\n",
        "            print(f\"Folder '{directory}' deleted successfully.\")\n",
        "        except OSError as e:\n",
        "            print(f\"Error deleting folder '{directory}': {e}\")\n",
        "\n",
        "    # Check if the directory exists\n",
        "    if not os.path.exists(directory):\n",
        "        # Create the directory\n",
        "        os.makedirs(directory)\n",
        "        print(f\"Directory '{directory}' created.\")\n",
        "    else:\n",
        "        print(f\"Directory '{directory}' already exists.\")\n",
        "    # Reload saved model and epoch number\n",
        "    if os.path.exists(f'./{directory}/{model_name}.pth'):\n",
        "        checkpoint = torch.load(f'./{directory}/{model_name}.pth')\n",
        "        self_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        start_epoch = checkpoint['epoch']\n",
        "        self_model.test_accuracy_list = checkpoint['test_accuracy_list']\n",
        "        print(\"Successfully reloaded model checkpoint!\")\n",
        "    else:\n",
        "        print(\"Model checkpoint not found. Starting from the beginning...\")\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "    model = None\n",
        "    if torch.cuda.is_available():\n",
        "        num_of_gpus = torch.cuda.device_count()\n",
        "        gpu_list = list(range(num_of_gpus))\n",
        "        model = nn.DataParallel(self_model, device_ids=gpu_list).to(self_model.device)\n",
        "    # Reload saved model and epoch number\n",
        "    if os.path.exists(f'./{directory}/best_accuracy_{model_name}.pth'):\n",
        "        best_model_state_dict = torch.load(f'./{directory}/best_accuracy_{model_name}.pth')\n",
        "        best_val_accuracy = best_model_state_dict['test_accuracy_list'][-1]  # Track the best validation accuracy\n",
        "        print(\"Successfully reloaded best model checkpoint!\")\n",
        "    else:\n",
        "        print(\"Best model checkpoint not found.\")\n",
        "        best_val_accuracy = 0.0  # Track the best validation accuracy\n",
        "        best_model_state_dict = None  # Track the state_dict of the best model\n",
        "    model.to(self_model.device)\n",
        "    return model, best_val_accuracy, best_model_state_dict, start_epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CuS4iTzBj3iP"
      },
      "outputs": [],
      "source": [
        "def one_hot_encode(x, num_classes):\n",
        "    vec = [0.0] * num_classes\n",
        "    vec[x] = 1.0\n",
        "    return vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZm-XzT81K2W"
      },
      "source": [
        "The victim model architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jg68u8E2jlzj"
      },
      "outputs": [],
      "source": [
        "class Alexnet(nn.Module):\n",
        "    def __init__(self, name=\"surrogate_model\", n_outputs=10):\n",
        "        super(Alexnet, self).__init__()\n",
        "\n",
        "        self.name = name\n",
        "        self.num_classes = n_outputs\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 48, 5, stride=1, padding=2)\n",
        "        self.conv1.bias.data.normal_(0, 0.01)\n",
        "        self.conv1.bias.data.fill_(0)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.lrn = nn.LocalResponseNorm(2)\n",
        "        self.pad = nn.MaxPool2d(3, stride=2)\n",
        "\n",
        "        self.batch_norm1 = nn.BatchNorm2d(48, eps=0.001)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(48, 128, 5, stride=1, padding=2)\n",
        "        self.conv2.bias.data.normal_(0, 0.01)\n",
        "        self.conv2.bias.data.fill_(1.0)\n",
        "\n",
        "        self.batch_norm2 = nn.BatchNorm2d(128, eps=0.001)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(128, 192, 3, stride=1, padding=1)\n",
        "        self.conv3.bias.data.normal_(0, 0.01)\n",
        "        self.conv3.bias.data.fill_(0)\n",
        "\n",
        "        self.batch_norm3 = nn.BatchNorm2d(192, eps=0.001)\n",
        "\n",
        "        self.conv4 = nn.Conv2d(192, 192, 3, stride=1, padding=1)\n",
        "        self.conv4.bias.data.normal_(0, 0.01)\n",
        "        self.conv4.bias.data.fill_(1.0)\n",
        "\n",
        "        self.batch_norm4 = nn.BatchNorm2d(192, eps=0.001)\n",
        "\n",
        "        self.conv5 = nn.Conv2d(192, 128, 3, stride=1, padding=1)\n",
        "        self.conv5.bias.data.normal_(0, 0.01)\n",
        "        self.conv5.bias.data.fill_(1.0)\n",
        "\n",
        "        self.batch_norm5 = nn.BatchNorm2d(128, eps=0.001)\n",
        "\n",
        "        self.fc1 = nn.Linear(1152, 512)\n",
        "        self.fc1.bias.data.normal_(0, 0.01)\n",
        "        self.fc1.bias.data.fill_(0)\n",
        "\n",
        "        self.drop = nn.Dropout(p=0.5)\n",
        "\n",
        "        self.batch_norm6 = nn.BatchNorm1d(512, eps=0.001)\n",
        "\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc2.bias.data.normal_(0, 0.01)\n",
        "        self.fc2.bias.data.fill_(0)\n",
        "\n",
        "        self.batch_norm7 = nn.BatchNorm1d(256, eps=0.001)\n",
        "\n",
        "        self.fc3 = nn.Linear(256, 10)\n",
        "        self.fc3.bias.data.normal_(0, 0.01)\n",
        "        self.fc3.bias.data.fill_(0)\n",
        "\n",
        "        self.soft = nn.Softmax()\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.test_accuracy_list = []\n",
        "        transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                        transforms.Normalize((0.5,), (0.5,))])\n",
        "        testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "        self.testloader = DataLoader(testset, batch_size=512, shuffle=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if isinstance(x, np.ndarray):\n",
        "            # Convert NumPy array to PyTorch tensor\n",
        "            x = torch.tensor(x)\n",
        "        x = x.to(self.device)\n",
        "        layer1 = self.batch_norm1(self.pad(self.lrn(self.relu(self.conv1(x)))))\n",
        "        layer2 = self.batch_norm2(self.pad(self.lrn(self.relu(self.conv2(layer1)))))\n",
        "        layer3 = self.batch_norm3(self.relu(self.conv3(layer2)))\n",
        "        layer4 = self.batch_norm4(self.relu(self.conv4(layer3)))\n",
        "        layer5 = self.batch_norm5(self.pad(self.relu(self.conv5(layer4))))\n",
        "        flatten = layer5.view(-1, 128 * 3 * 3)\n",
        "        fully1 = self.relu(self.fc1(flatten))\n",
        "        fully1 = self.batch_norm6(self.drop(fully1))\n",
        "        fully2 = self.relu(self.fc2(fully1))\n",
        "        fully2 = self.batch_norm7(self.drop(fully2))\n",
        "        logits = self.fc3(fully2)\n",
        "        return logits\n",
        "\n",
        "    def test_model(self):\n",
        "        # Test the model\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        model = None\n",
        "        if torch.cuda.is_available():\n",
        "            num_of_gpus = torch.cuda.device_count()\n",
        "            gpu_list = list(range(num_of_gpus))\n",
        "            model = nn.DataParallel(self, device_ids=gpu_list).to(self.device)\n",
        "        # Don't need to keep track of gradients\n",
        "        with torch.no_grad():\n",
        "            for images, labels in self.testloader:\n",
        "                images = images.view(images.shape[0], 3, 32, 32).detach().clone()\n",
        "                if torch.cuda.is_available():\n",
        "                    outputs = model(images)\n",
        "                else:\n",
        "                    outputs = self(images)\n",
        "                images, labels, outputs = images.to(self.device), labels.to(self.device), outputs.to(self.device)\n",
        "                _, predicted = torch.max(outputs, dim=1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        print(f\"The accuracy of the victim model f is: {100 * correct / total}\")\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "        return correct / total\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeKUyWg31RNx"
      },
      "source": [
        "The surrogate model architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aeXj3pmDjmhL"
      },
      "outputs": [],
      "source": [
        "class AlexnetSurrogate(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(AlexnetSurrogate, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 48, kernel_size=5, stride=1, padding=2)\n",
        "        self.conv1.bias.data.normal_(0, 0.01)\n",
        "        self.conv1.bias.data.fill_(0)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.lrn = nn.LocalResponseNorm(2)\n",
        "        self.pad = nn.MaxPool2d(3, stride=2)\n",
        "        self.batch_norm1 = nn.BatchNorm2d(48, eps=0.001)\n",
        "\n",
        "        self.layer1 = self.make_residue_block(48, 128, stride=1)\n",
        "        self.layer2 = self.make_residue_block(128, 192, stride=2)\n",
        "        self.layer3 = self.make_residue_block(192, 192, stride=1)\n",
        "        self.layer4 = self.make_residue_block(192, 128, stride=2)\n",
        "\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc1 = nn.Linear(128, 512)\n",
        "        self.fc1.bias.data.normal_(0, 0.01)\n",
        "        self.fc1.bias.data.fill_(0)\n",
        "        self.drop = nn.Dropout(p=0.5)\n",
        "        self.batch_norm2 = nn.BatchNorm1d(512, eps=0.001)\n",
        "\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc2.bias.data.normal_(0, 0.01)\n",
        "        self.fc2.bias.data.fill_(0)\n",
        "        self.batch_norm3 = nn.BatchNorm1d(256, eps=0.001)\n",
        "\n",
        "        self.fc3 = nn.Linear(256, num_classes)\n",
        "        self.fc3.bias.data.normal_(0, 0.01)\n",
        "        self.fc3.bias.data.fill_(0)\n",
        "\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.test_accuracy_list = []\n",
        "\n",
        "        transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                        transforms.Normalize((0.5,), (0.5,))])\n",
        "        testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "        self.testloader = DataLoader(testset, batch_size=1024, shuffle=False)\n",
        "\n",
        "    def make_residue_block(self, in_channels, out_channels, stride):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.lrn(x)\n",
        "        x = self.pad(x)\n",
        "        x = self.batch_norm1(x)\n",
        "\n",
        "        # Residue blocks\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avg_pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        x = self.fc1(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.batch_norm2(x)\n",
        "\n",
        "        x = self.fc2(x)\n",
        "        x = self.batch_norm3(x)\n",
        "\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def train_model(self, train_loader, criterion, optimizer, n_epochs=10, print_every=500,\n",
        "                    model_name=\"Alexnet_surrogate_model\"):\n",
        "        model, best_val_accuracy, best_model_state_dict, start_epoch = prepare_for_training(self, model_name, optimizer)\n",
        "        for epoch in range(start_epoch, n_epochs):\n",
        "            running_loss = 0.0\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            start_time = time.time()\n",
        "            start_time_epoch = time.time()\n",
        "            for i, (inputs, labels) in enumerate(train_loader, 0):\n",
        "                inputs = inputs.view(inputs.shape[0], 3, 32, 32).detach().clone()\n",
        "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
        "                if torch.cuda.is_available():\n",
        "                    outputs = model(inputs)\n",
        "                else:\n",
        "                    outputs = self(inputs)\n",
        "                try:\n",
        "                    labels = labels.view(-1, outputs.size()[1]).float().detach().clone().requires_grad_(True)\n",
        "                except:\n",
        "                    labels = torch.tensor(list(\n",
        "                        map(lambda x: one_hot_encode(x, outputs.size()[1]), labels))).detach().clone().requires_grad_(\n",
        "                        True)\n",
        "                outputs, labels = outputs.to(self.device), labels.to(self.device)\n",
        "                loss = criterion(outputs, labels)  # + sum_fitnesses\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                running_loss += loss.item()\n",
        "\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                _, labels = torch.max(labels.data, 1)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "                if i % print_every == print_every - 1:\n",
        "                    finish_time = time.time()\n",
        "                    total_time = finish_time - start_time\n",
        "                    print('[%d, %5d] loss: %.3f accuracy: %.3f the time it took: %.3f seconds' % (\n",
        "                        epoch + 1, i + 1, running_loss / print_every, 100 * correct / total, total_time))\n",
        "                    running_loss = 0.0\n",
        "                    correct = 0\n",
        "                    total = 0\n",
        "                    start_time = time.time()\n",
        "            finish_time_epoch = time.time()\n",
        "            total_time_epoch = finish_time_epoch - start_time_epoch\n",
        "            print(f'Epoch {epoch + 1} took {total_time_epoch} seconds')\n",
        "            validation_accuracy = self.validate_model()\n",
        "            self.test_accuracy_list.append(validation_accuracy)\n",
        "            # Save model after each epoch\n",
        "            checkpoint = {\n",
        "                'epoch': epoch + 1,\n",
        "                'model_state_dict': self.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'test_accuracy_list': self.test_accuracy_list\n",
        "            }\n",
        "            directory = f\"checkpoints/{self.__class__.__name__}\"\n",
        "            torch.save(checkpoint, f'./{directory}/{model_name}.pth')\n",
        "            if validation_accuracy > best_val_accuracy:\n",
        "                best_val_accuracy = validation_accuracy\n",
        "                best_model_state_dict = {\n",
        "                    'epoch': epoch + 1,\n",
        "                    'model_state_dict': self.state_dict(),\n",
        "                    'optimizer_state_dict': optimizer.state_dict(),\n",
        "                    'test_accuracy_list': self.test_accuracy_list\n",
        "                }\n",
        "                torch.save(best_model_state_dict, f'./{directory}/best_accuracy_{model_name}.pth')\n",
        "            print(\"Saved model checkpoint!\")\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "        print(\n",
        "            f\"The maximal accuracy during training was: {max(self.test_accuracy_list)} on epoch: {self.test_accuracy_list.index(max(self.test_accuracy_list))}\")\n",
        "        # self.plot_accuracy_graph()\n",
        "\n",
        "    def plot_accuracy_graph(self):\n",
        "        accuracy_list = self.test_accuracy_list\n",
        "        # Plotting using Seaborn\n",
        "        sns.set(style=\"darkgrid\")\n",
        "        sns.lineplot(x=range(len(accuracy_list)), y=accuracy_list, marker='X')\n",
        "\n",
        "        # Set labels and title\n",
        "        plt.xlabel(\"Epoch\")\n",
        "        plt.ylabel(\"Accuracy\")\n",
        "        plt.title(\"Accuracy Over Epochs\")\n",
        "\n",
        "        # Display the plot\n",
        "        plt.show()\n",
        "\n",
        "    def validate_model(self):\n",
        "        # Test the model\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        model = None\n",
        "        if torch.cuda.is_available():\n",
        "            num_of_gpus = torch.cuda.device_count()\n",
        "            gpu_list = list(range(num_of_gpus))\n",
        "            model = nn.DataParallel(self, device_ids=gpu_list).to(self.device)\n",
        "        # Don't need to keep track of gradients\n",
        "        with torch.no_grad():\n",
        "            for images, labels in self.testloader:\n",
        "                images = images.view(images.shape[0], 3, 32, 32).detach().clone()\n",
        "                if torch.cuda.is_available():\n",
        "                    outputs = model(images)\n",
        "                else:\n",
        "                    outputs = self(images)\n",
        "                images, labels, outputs = images.to(self.device), labels.to(self.device), outputs.to(self.device)\n",
        "                _, predicted = torch.max(outputs, dim=1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        print(f\"Accuracy on test set is: {100 * correct / total}\")\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "        return correct / total\n",
        "\n",
        "    def test_model(self):\n",
        "        # Test the model\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        model = None\n",
        "        if torch.cuda.is_available():\n",
        "            num_of_gpus = torch.cuda.device_count()\n",
        "            gpu_list = list(range(num_of_gpus))\n",
        "            model = nn.DataParallel(self, device_ids=gpu_list).to(self.device)\n",
        "        # Don't need to keep track of gradients\n",
        "        with torch.no_grad():\n",
        "            for images, labels in self.testloader:\n",
        "                images = images.view(images.shape[0], 3, 32, 32).detach().clone()\n",
        "                if torch.cuda.is_available():\n",
        "                    outputs = model(images)\n",
        "                else:\n",
        "                    outputs = self(images)\n",
        "                images, labels, outputs = images.to(self.device), labels.to(self.device), outputs.to(self.device)\n",
        "                _, predicted = torch.max(outputs, dim=1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        print(f\"The accuracy of the substitute (stolen) model f' is: {100 * correct / total}\")\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "        return correct / total\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMDthPOSzNhD"
      },
      "source": [
        "Custom DataLoaders for Retrieving Saved Data from Disk.\n",
        "For colab we load the data from disk due to RAM limitations (12.7GB!). This is done using the `DatasetLoader` class. If you use this code on better hardware, you should\n",
        "\n",
        "1. use the `SmallDatasetLoader` to improve runtime.\n",
        "2. set `small_dataset=True` when you call `main_algorithm`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9zsmbMv7hQTo"
      },
      "outputs": [],
      "source": [
        "class SmallDatasetLoader(Dataset):\n",
        "    def __init__(self, data_dir, file_size=20000):\n",
        "        # Convert the list of tuples to a list of tensors\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "            # Load the data tensor from the file\n",
        "        self.data_dir = data_dir\n",
        "        file_list = os.listdir(self.data_dir)\n",
        "        data_list = sorted([x for x in file_list if \"_input\" in x])\n",
        "        labels_list = sorted([x for x in file_list if \"_labels\" in x])\n",
        "        self.tensor_list = []\n",
        "        self.label_list = []\n",
        "        for data_file_name, labels_file_name in zip(data_list, labels_list):\n",
        "            data_file_path = os.path.join(self.data_dir, data_file_name)\n",
        "            labels_file_path = os.path.join(self.data_dir, labels_file_name)\n",
        "            data = np.load(data_file_path, mmap_mode='r')\n",
        "            labels = np.load(labels_file_path, mmap_mode='r')\n",
        "            data = torch.split(torch.tensor(data), split_size_or_sections=1, dim=0)\n",
        "            labels = list(torch.split(torch.tensor(labels), split_size_or_sections=1, dim=0))\n",
        "            shape = data[0].shape\n",
        "            shape_labels = labels[0].shape\n",
        "            data = [x.view(-1, shape[2], shape[3], shape[4]) for x in data]\n",
        "            labels = [x.view(-1, shape_labels[2]) for x in labels]\n",
        "            self.tensor_list.extend(data)\n",
        "            self.label_list.extend(labels)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tensor_list)\n",
        "\n",
        "    def add_data(self, cur_tensor_list, cur_label_list):\n",
        "        self.tensor_list.extend(cur_tensor_list)\n",
        "        self.label_list.extend(cur_label_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        data, label = self.tensor_list[idx], self.label_list[idx]\n",
        "\n",
        "        return data, label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3aDY2vCJhQfA"
      },
      "outputs": [],
      "source": [
        "class DatasetLoader(Dataset):\n",
        "    def __init__(self, data_dir, file_size=20000, transform=None, subset_generations=None):\n",
        "        self.data_dir = data_dir\n",
        "        self.transform = transform\n",
        "        file_list = os.listdir(data_dir)\n",
        "        self.data_list = sorted([x for x in file_list if \"_input\" in x])\n",
        "        self.labels_list = sorted([x for x in file_list if \"_labels\" in x])\n",
        "        if subset_generations is not None:\n",
        "            away_data = sorted([x for x in self.data_list if \"Away_\" in x], key=lambda y: int(y.split(\"_\")[-2]))\n",
        "            away_labels = sorted([x for x in self.labels_list if \"Away_\" in x], key=lambda y: int(y.split(\"_\")[-2]))\n",
        "            toward_data = sorted([x for x in self.data_list if \"Toward_\" in x], key=lambda y: int(y.split(\"_\")[-2]))\n",
        "            toward_labels = sorted([x for x in self.labels_list if \"Toward_\" in x], key=lambda y: int(y.split(\"_\")[-2]))\n",
        "            self.data_list = away_data[:subset_generations] + toward_data[:subset_generations]\n",
        "            self.labels_list = away_labels[:subset_generations] + toward_labels[:subset_generations]\n",
        "        self.file_size = file_size\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return the total number of tensors in all files\n",
        "        return len(self.data_list) * self.file_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        file_idx = idx // self.file_size  # Calculate which file to load\n",
        "        sample_idx = idx % self.file_size  # Calculate the index within the file\n",
        "\n",
        "        data_file_name = self.data_list[file_idx]\n",
        "        labels_file_name = self.labels_list[file_idx]\n",
        "        data_file_path = os.path.join(self.data_dir, data_file_name)\n",
        "        labels_file_path = os.path.join(self.data_dir, labels_file_name)\n",
        "\n",
        "        # Load the data tensor from the file\n",
        "        data = np.load(data_file_path, mmap_mode='r')  # Use mmap_mode to avoid loading the entire file into memory\n",
        "        labels = np.load(labels_file_path, mmap_mode='r')  # Use mmap_mode to avoid loading the entire file into memory\n",
        "\n",
        "        # Extract the individual tensor at the specified index\n",
        "        data = data[sample_idx]\n",
        "        labels = labels[sample_idx]\n",
        "\n",
        "        # You can apply transformations here if needed\n",
        "        if self.transform:\n",
        "            # Convert to PIL Image\n",
        "            data_size = data.shape\n",
        "            pil_image = torch.tensor(data.squeeze(0)).permute(1, 2, 0).numpy()\n",
        "            pil_image = self.transform(pil_image)\n",
        "            pil_image.view(data_size)\n",
        "            # Convert back to tensor\n",
        "            data = pil_image\n",
        "\n",
        "        return data, labels\n",
        "\n",
        "    def set_transform(self, transform):\n",
        "        self.transform = transform\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLb4mOnpV1LY"
      },
      "source": [
        "# 3. BAM: The attack algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsQAOgwmzDW7"
      },
      "source": [
        "The main code for the ES algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TDSW_T_Ifip7"
      },
      "outputs": [],
      "source": [
        "class BasicModelGeneticAlgorithm:\n",
        "    def __init__(self, model, random_data_generator_function, num_of_classes, model_name):\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.model = model.to(self.device)\n",
        "        if torch.cuda.is_available():\n",
        "            num_of_gpus = torch.cuda.device_count()\n",
        "            gpu_list = list(range(num_of_gpus))\n",
        "            self.model = nn.DataParallel(self.model, device_ids=gpu_list)\n",
        "        self.query_counter = 0\n",
        "        self.avg_fitness = []\n",
        "        self.random_data_generator_function = random_data_generator_function\n",
        "        self.num_of_classes = num_of_classes\n",
        "        self.model_name = model_name\n",
        "\n",
        "    # Define the fitness function\n",
        "    def fitness(self, population, epsilon):\n",
        "        \"\"\"\n",
        "        This function receives an output from the target model and evaluate its fitness (we want that the output confidence\n",
        "        will be as close to uniform distribution as possible)\n",
        "        :rtype: float - the fittness of the record\n",
        "        \"\"\"\n",
        "        predictions_tensor = torch.cat([ind[1] - epsilon for ind in population])\n",
        "        max_probs, _ = torch.max(predictions_tensor, dim=1)\n",
        "        new_max_probs = torch.max(max_probs, torch.tensor(0))\n",
        "        list_of_fitnesses = [x.item() for x in torch.split(new_max_probs, split_size_or_sections=1, dim=0)]\n",
        "        return list_of_fitnesses\n",
        "\n",
        "    # Generate a random population of individuals\n",
        "    def generate_population(self, size, generations):\n",
        "        random_data = self.random_data_generator_function(size)\n",
        "        population = self.predict_and_create_proxy_dataset(random_data, 0, generations)\n",
        "        return population\n",
        "\n",
        "    # Select individuals for the next generation\n",
        "    def select(self, population, fitnesses, k):\n",
        "        fitnesses_weights = [(i, f) for i, f in enumerate(fitnesses)]\n",
        "        # sort by fitness and select the k individuals with the lowest fitness\n",
        "        sorted_fitnesses = sorted(fitnesses_weights, key=lambda x: x[1])\n",
        "        top_k_fitness = sorted_fitnesses[:k]\n",
        "\n",
        "        top_k_population = [population[x[0]] for x in top_k_fitness]\n",
        "        top_k_population_with_fitness = [(population[x[0]], x[1]) for x in top_k_fitness]\n",
        "        return top_k_population, top_k_population_with_fitness\n",
        "\n",
        "    # Run the genetic algorithm\n",
        "    def run_genetic_algorithm(self, generations, k, epsilon, population_size, search_spread, non_2d):\n",
        "        population = self.generate_population(population_size, generations)\n",
        "        for generation in range(generations):\n",
        "            gc.collect()\n",
        "            start_time = time.time()\n",
        "            fitnesses = self.fitness(population, epsilon)\n",
        "            fitness_avg = mean(fitnesses)\n",
        "\n",
        "            self.avg_fitness.append(fitness_avg)\n",
        "            top_k_population, top_k_population_with_fitness = self.select(population, fitnesses, k)\n",
        "            gc.collect()\n",
        "            new_population = self.create_new_generation_with_noise(top_k_population, population,\n",
        "                                                                   population_size, search_spread)\n",
        "            gc.collect()\n",
        "            population = self.predict_and_create_proxy_dataset(new_population, generation, generations)\n",
        "            gc.collect()\n",
        "            finish_time = time.time()\n",
        "            total_time = finish_time - start_time\n",
        "            print(\n",
        "                f\"Generation number: {generation}, The average fitness: {fitness_avg:.5f}, The time it took was: {total_time:.3f}seconds\")\n",
        "            if not non_2d:\n",
        "                x_data = torch.cat([x[0] for x in population], dim=0).to(self.device)\n",
        "                y_data = torch.cat([x[1].view(-1, 2) for x in population], dim=0).to(self.device)\n",
        "                if isinstance(self.model, nn.DataParallel):\n",
        "                    self.model.module.plot_decision_boundary(x_data, y_data)\n",
        "                else:\n",
        "                    self.model.plot_decision_boundary(x_data, y_data)\n",
        "        return population\n",
        "\n",
        "    def predict_and_create_proxy_dataset(self, population, cur_generation, generations):\n",
        "        # Define the desired batch size\n",
        "        max_batch_size = Config.instance[\"genetic_alg_prediction_max_batch_size\"]\n",
        "        batch_size = max_batch_size if len(population) > max_batch_size else len(population)\n",
        "        num_samples = len(population)\n",
        "\n",
        "        model_prediction_list = []\n",
        "\n",
        "        for i in range(0, num_samples, batch_size):\n",
        "            batch = torch.cat(population[i:i + batch_size], dim=0).to(self.device)\n",
        "            batch_prediction = self.model(batch).detach()\n",
        "            model_prediction_list.append(batch_prediction)\n",
        "\n",
        "        model_prediction = torch.cat(model_prediction_list, dim=0)\n",
        "        self.query_counter += model_prediction.shape[0]\n",
        "\n",
        "        list_of_confidence = list(torch.split(model_prediction, split_size_or_sections=1, dim=0))\n",
        "        cur_dataset = [(torch.tensor(population[i]), list_of_confidence[i]) for i in range(len(list_of_confidence))]\n",
        "        sub_folder = \"Toward\"\n",
        "        data_directory = Config.instance[\"data_directory\"]\n",
        "        directory = f\"{data_directory}/{self.model_name}/{generations}_generations\"\n",
        "        # Check if the directory exists\n",
        "        if not os.path.exists(directory):\n",
        "            # Create the directory\n",
        "            os.makedirs(directory)\n",
        "            print(f\"Directory '{directory}' created.\")\n",
        "        file_path_confidence = f\"{directory}/{sub_folder}_proxy_dataset_confidence_{generations}_batch_{cur_generation}\"\n",
        "        save_tensor_list_to_file(cur_dataset, file_path_confidence)\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "        return cur_dataset\n",
        "\n",
        "    def create_new_generation_with_noise(self, top_k_population, population, population_size, search_spread):\n",
        "        new_population = [x[0] for x in top_k_population]\n",
        "        cur_population_size = len(top_k_population)\n",
        "        max_values, _ = torch.max(torch.stack([x[0][0] for x in population]), dim=0)\n",
        "        min_values, _ = torch.min(torch.stack([x[0][0] for x in population]), dim=0)\n",
        "        ss_vector = ((max_values - min_values) / search_spread).detach()\n",
        "        population_tensor = torch.cat(new_population)\n",
        "        population_original_shape = tuple(population_tensor.shape)\n",
        "        factor = math.ceil((float(population_size) / float(cur_population_size)))\n",
        "        new_population_list = []\n",
        "        noise_size = 2\n",
        "        for i in range(factor - 1):\n",
        "            random_number = random.choice([1, -1])\n",
        "            new_population_list.append(random_number * (noise_size * ss_vector * torch.rand(\n",
        "                population_original_shape) - 0.5 * noise_size * ss_vector) + population_tensor)\n",
        "        splitted_population_list = [list(torch.split(x, 1, dim=0)) for x in new_population_list]\n",
        "        flattened_list = [item for sublist in splitted_population_list for item in sublist]\n",
        "        splitted_population = flattened_list + new_population\n",
        "        new_population = random.sample(splitted_population, population_size)\n",
        "        return new_population\n",
        "\n",
        "    def update_query_counter(self, amount):\n",
        "        self.query_counter += amount\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZunlupGzfXC"
      },
      "source": [
        "The code for training $f'$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ult8YcBXhpyB"
      },
      "outputs": [],
      "source": [
        "def train_surrogate_model_generic(dataloader, num_epochs, model_class, criterion, optimizer_name=\"AdamW\"):\n",
        "    \"\"\"\n",
        "    :param extracted_dataset:\n",
        "    :param num_epochs:\n",
        "    :param model_class: this model class must have train and test methods implemented in it.\n",
        "    :param criterion:\n",
        "    :return: Trained surrogate model that is a copy of the target model\n",
        "    \"\"\"\n",
        "    surrogate_model = model_class()\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    surrogate_model.to(device)\n",
        "    if optimizer_name == \"Adam\":\n",
        "        optimizer = optim.Adam(surrogate_model.parameters(), lr=1e-2)\n",
        "    elif optimizer_name == \"SGD\":\n",
        "        optimizer = optim.SGD(surrogate_model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
        "    elif optimizer_name == \"RMSprop\":\n",
        "        optimizer = optim.RMSprop(surrogate_model.parameters(), lr=1e-2, alpha=0.9, momentum=0.5)\n",
        "    else:\n",
        "        optimizer = optim.AdamW(surrogate_model.parameters(), lr=5e-3)\n",
        "\n",
        "    # Train the model\n",
        "    start_time = time.time()\n",
        "    surrogate_model.train_model(dataloader, criterion, optimizer, n_epochs=num_epochs)\n",
        "    finish_time = time.time()\n",
        "    total_time = finish_time - start_time\n",
        "    print(f\"The time it took to train the model was: {total_time}seconds\")\n",
        "\n",
        "    return surrogate_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsfyrN_f0Rqs"
      },
      "source": [
        "The main BAM code: runs the ES algorithm and then trains $f'$ on the extracted data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r5KP0UF6hQio"
      },
      "outputs": [],
      "source": [
        "def main_algorithm(model, model_class, criterion, random_data_generator_function, num_of_classes,\n",
        "                    k=300, epsilon=0.05, population_size=1000, generations=20, search_spread=10, epochs=50,\n",
        "                    optimizer_name=\"AdamW\", non_2d=True, small_dataset=False, save_path=None):\n",
        "    data_directory = Config.instance[\"data_directory\"]\n",
        "    destination_folder = Config.instance[\"destination_folder\"].format(data_directory=data_directory,\n",
        "                                                                      model_class=f\"{model_class.__name__}\",\n",
        "                                                                      generations=f\"{generations}\")\n",
        "    file_path_confidence_batch = Config.instance[\"file_path_confidence_batch\"].format(\n",
        "        destination_folder=destination_folder, generations=f\"{generations}\")\n",
        "    if Config.instance[\"dont_get_from_dist\"]:\n",
        "        import shutil\n",
        "        try:\n",
        "            shutil.rmtree(destination_folder)\n",
        "            print(f\"Folder '{destination_folder}' deleted successfully.\")\n",
        "        except OSError as e:\n",
        "            print(f\"Error deleting folder '{destination_folder}': {e}\")\n",
        "    # Here we are using Evolutionary algorithm in order to extract data to train copy model\n",
        "    ga = BasicModelGeneticAlgorithm(model, random_data_generator_function, num_of_classes, model_class.__name__)\n",
        "    if not os.path.exists(file_path_confidence_batch) or Config.instance[\"dont_get_from_dist\"]:\n",
        "        best_individuals1 = ga.run_genetic_algorithm(generations, k, epsilon, population_size, search_spread, non_2d)\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "        print(f\"The number of queries was: {ga.query_counter}\")\n",
        "\n",
        "    batch_size = Config.instance[\"batch_size\"]\n",
        "    if small_dataset:\n",
        "        dataset = SmallDatasetLoader(destination_folder, file_size=population_size)\n",
        "    else:\n",
        "        dataset = DatasetLoader(destination_folder, file_size=population_size)\n",
        "\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=16)\n",
        "\n",
        "    print(f\"Now we are training the model with the data extracted by using evolutionary algorithms:\")\n",
        "    if non_2d:\n",
        "        Ea_no_knn_surrogate_model = train_surrogate_model_generic(dataloader, epochs, model_class, criterion,\n",
        "                                                                  optimizer_name=optimizer_name)\n",
        "        print(\"\\nThe final results are:\")\n",
        "        model_acc = Ea_no_knn_surrogate_model.test_model()\n",
        "    attack_success_rate = test_trans(Ea_no_knn_surrogate_model, model, criterion, num_of_classes, model.testloader)\n",
        "    print(f\"The Attack Success Rate for PGD samples generated f' and used on f (transfer) is: {attack_success_rate}\")\n",
        "    # Save model after each epoch\n",
        "\n",
        "    if Ea_no_knn_surrogate_model.__class__.__name__ == 'DataParallel':\n",
        "        checkpoint = {\n",
        "            'model_state_dict': Ea_no_knn_surrogate_model.module.state_dict()\n",
        "        }\n",
        "    else:\n",
        "        checkpoint = {\n",
        "            'model_state_dict': Ea_no_knn_surrogate_model.state_dict(),\n",
        "        }\n",
        "    if save_path is None:\n",
        "        model_name = \"phase1\"\n",
        "        directory = f\"checkpoints/phases_tests/{Ea_no_knn_surrogate_model.__class__.__name__}\"\n",
        "    else:\n",
        "        model_name = save_path\n",
        "        directory = f\"checkpoints/model_test/{Ea_no_knn_surrogate_model.__class__.__name__}\"\n",
        "\n",
        "    # Check if the directory exists\n",
        "    if not os.path.exists(directory):\n",
        "        # Create the directory\n",
        "        os.makedirs(directory)\n",
        "        print(f\"Directory '{directory}' created.\")\n",
        "    torch.save(checkpoint, f'./{directory}/{model_name}.pth')\n",
        "    # del proxy_dataset\n",
        "    del Ea_no_knn_surrogate_model\n",
        "    del dataset\n",
        "    # release all GPU memory\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "    return model_acc, attack_success_rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hd9t3CITV1dr"
      },
      "source": [
        "# 4. Evaluate BAM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XoyFlvAiMlm"
      },
      "source": [
        "Configuration settings - these include the essential hyperparameters and setup details required for the subsequent sections of the code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xF310wHsg7x5",
        "outputId": "c4b96e2d-a2d8-47db-b69e-c4e8adec3efa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:03<00:00, 45827233.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n"
          ]
        }
      ],
      "source": [
        "class Config:\n",
        "    instance = None\n",
        "    log = None\n",
        "\n",
        "    def __new__(cls, log_file=\"model_log.log\"):\n",
        "        if cls.instance is None:\n",
        "            ins = cls.create_instance()\n",
        "            cls.instance = ins\n",
        "        if cls.log is None:\n",
        "            cls.log = cls.configure_logging(log_file=log_file)\n",
        "\n",
        "    @staticmethod\n",
        "    def create_instance():\n",
        "        # Default config for Jupyter notebook\n",
        "        config = {\n",
        "            'k': 6000,\n",
        "            'epsilon': 0.0005,\n",
        "            'population_size': 20000,\n",
        "            'generations': 30, # Change this to 40 generations if you have resources more then the free version of colab\n",
        "            'search_spread': 10,\n",
        "            'epochs': 20, # Change this to 40 epoch if you have resources more then the free version of colab\n",
        "            'dont_get_from_dist': True,\n",
        "            'num_of_classes': 10,\n",
        "            'learning_rate': 0.3,\n",
        "            'optimizer_name': 'AdamW',\n",
        "            'batch_size': 64,\n",
        "            'delete_checkpoints': True,\n",
        "            'genetic_alg_prediction_max_batch_size': 500,\n",
        "            'data_directory': 'SaveDataset/Batches/colab',\n",
        "            'destination_folder': '{data_directory}/{model_class}/{generations}_generations',\n",
        "            'file_path_confidence_batch': '{destination_folder}/Away_proxy_dataset_confidence_{generations}_batch_0_input.npy',\n",
        "        }\n",
        "        return config\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def configure_logging(log_file=\"model_log.log\", log_level=\"INFO\"):\n",
        "        logger = logging.getLogger(\"my_logger\")\n",
        "        logger.setLevel(log_level)\n",
        "        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
        "        file_handler = logging.FileHandler(log_file)\n",
        "        file_handler.setFormatter(formatter)\n",
        "        logger.addHandler(file_handler)\n",
        "\n",
        "        return logger\n",
        "\n",
        "prepare_config()\n",
        "config = Config.instance\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "num_of_classes = config[\"num_of_classes\"]\n",
        "name = 'teacher_alexnet_for_cifar10'\n",
        "ckpt_path = 'teacher_alexnet_for_cifar10_state_dict'\n",
        "alex = Alexnet(name, num_of_classes)\n",
        "alex.load_state_dict(torch.load(ckpt_path, map_location=device))\n",
        "alex.to(device)\n",
        "k = config[\"k\"]\n",
        "epsilon = config[\"epsilon\"]\n",
        "population_size = config[\"population_size\"]\n",
        "generations = config[\"generations\"]\n",
        "search_spread = config[\"search_spread\"]\n",
        "epochs = config[\"epochs\"]\n",
        "criterion = nn.MSELoss().to(device)\n",
        "optimizer_name = config[\"optimizer_name\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orSxHCkQ2BqB"
      },
      "source": [
        "Execute BAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s4CaO65hhQlF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4efa143-4a4e-4d36-c459-948929d5dd39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error deleting folder 'SaveDataset/Batches/colab/AlexnetSurrogate/40_generations': [Errno 2] No such file or directory: 'SaveDataset/Batches/colab/AlexnetSurrogate/40_generations'\n",
            "Directory 'SaveDataset/Batches/colab/AlexnetSurrogate/40_generations' created.\n",
            "Generation number: 0, The average fitness: 10.75227, The time it took was: 9.104seconds\n",
            "Generation number: 1, The average fitness: 9.82730, The time it took was: 6.585seconds\n",
            "Generation number: 2, The average fitness: 9.70070, The time it took was: 6.895seconds\n",
            "Generation number: 3, The average fitness: 9.70349, The time it took was: 6.516seconds\n",
            "Generation number: 4, The average fitness: 9.77393, The time it took was: 7.228seconds\n",
            "Generation number: 5, The average fitness: 9.78873, The time it took was: 6.649seconds\n",
            "Generation number: 6, The average fitness: 9.85808, The time it took was: 6.968seconds\n",
            "Generation number: 7, The average fitness: 9.89243, The time it took was: 6.985seconds\n",
            "Generation number: 8, The average fitness: 9.91640, The time it took was: 6.695seconds\n",
            "Generation number: 9, The average fitness: 9.95958, The time it took was: 7.295seconds\n",
            "Generation number: 10, The average fitness: 9.99753, The time it took was: 6.835seconds\n",
            "Generation number: 11, The average fitness: 10.06601, The time it took was: 6.987seconds\n",
            "Generation number: 12, The average fitness: 10.04518, The time it took was: 6.911seconds\n",
            "Generation number: 13, The average fitness: 10.09207, The time it took was: 6.597seconds\n",
            "Generation number: 14, The average fitness: 10.06399, The time it took was: 7.598seconds\n",
            "Generation number: 15, The average fitness: 10.08818, The time it took was: 6.914seconds\n",
            "Generation number: 16, The average fitness: 10.10178, The time it took was: 6.903seconds\n",
            "Generation number: 17, The average fitness: 10.13376, The time it took was: 6.842seconds\n",
            "Generation number: 18, The average fitness: 10.13753, The time it took was: 6.680seconds\n",
            "Generation number: 19, The average fitness: 10.11591, The time it took was: 7.219seconds\n",
            "Generation number: 20, The average fitness: 10.13727, The time it took was: 6.777seconds\n",
            "Generation number: 21, The average fitness: 10.15111, The time it took was: 6.970seconds\n",
            "Generation number: 22, The average fitness: 10.14230, The time it took was: 6.956seconds\n",
            "Generation number: 23, The average fitness: 10.14889, The time it took was: 7.576seconds\n",
            "Generation number: 24, The average fitness: 10.10540, The time it took was: 7.189seconds\n",
            "Generation number: 25, The average fitness: 10.13626, The time it took was: 7.206seconds\n",
            "Generation number: 26, The average fitness: 10.14285, The time it took was: 6.755seconds\n",
            "Generation number: 27, The average fitness: 10.13391, The time it took was: 7.070seconds\n",
            "Generation number: 28, The average fitness: 10.13180, The time it took was: 6.799seconds\n",
            "Generation number: 29, The average fitness: 10.12811, The time it took was: 7.193seconds\n",
            "Generation number: 30, The average fitness: 10.13564, The time it took was: 7.134seconds\n",
            "Generation number: 31, The average fitness: 10.13302, The time it took was: 6.744seconds\n",
            "Generation number: 32, The average fitness: 10.14926, The time it took was: 7.128seconds\n",
            "Generation number: 33, The average fitness: 10.16870, The time it took was: 6.673seconds\n",
            "Generation number: 34, The average fitness: 10.15761, The time it took was: 7.171seconds\n",
            "Generation number: 35, The average fitness: 10.16662, The time it took was: 7.176seconds\n",
            "Generation number: 36, The average fitness: 10.14527, The time it took was: 6.745seconds\n",
            "Generation number: 37, The average fitness: 10.13933, The time it took was: 6.964seconds\n",
            "Generation number: 38, The average fitness: 10.12883, The time it took was: 6.678seconds\n",
            "Generation number: 39, The average fitness: 10.13544, The time it took was: 7.064seconds\n",
            "The number of queries was: 820000\n",
            "Now we are training the model with the data extracted by using evolutionary algorithms:\n",
            "Files already downloaded and verified\n",
            "Error deleting folder 'checkpoints/AlexnetSurrogate': [Errno 2] No such file or directory: 'checkpoints/AlexnetSurrogate'\n",
            "Directory 'checkpoints/AlexnetSurrogate' created.\n",
            "Model checkpoint not found. Starting from the beginning...\n",
            "Best model checkpoint not found.\n",
            "[1,   500] loss: 50.856 accuracy: 13.125 the time it took: 18.319 seconds\n",
            "[1,  1000] loss: 48.686 accuracy: 16.091 the time it took: 16.408 seconds\n",
            "[1,  1500] loss: 48.463 accuracy: 16.284 the time it took: 15.204 seconds\n",
            "[1,  2000] loss: 48.183 accuracy: 17.072 the time it took: 15.222 seconds\n",
            "[1,  2500] loss: 48.363 accuracy: 16.550 the time it took: 15.199 seconds\n",
            "[1,  3000] loss: 48.013 accuracy: 17.381 the time it took: 15.293 seconds\n",
            "[1,  3500] loss: 47.830 accuracy: 17.116 the time it took: 16.186 seconds\n",
            "[1,  4000] loss: 47.995 accuracy: 16.431 the time it took: 15.342 seconds\n",
            "[1,  4500] loss: 47.220 accuracy: 18.188 the time it took: 15.211 seconds\n",
            "[1,  5000] loss: 46.597 accuracy: 18.616 the time it took: 15.277 seconds\n",
            "[1,  5500] loss: 46.207 accuracy: 18.856 the time it took: 15.134 seconds\n",
            "[1,  6000] loss: 45.868 accuracy: 19.550 the time it took: 15.832 seconds\n",
            "[1,  6500] loss: 45.440 accuracy: 20.484 the time it took: 15.787 seconds\n",
            "[1,  7000] loss: 45.415 accuracy: 19.913 the time it took: 15.261 seconds\n",
            "[1,  7500] loss: 45.286 accuracy: 20.084 the time it took: 15.204 seconds\n",
            "[1,  8000] loss: 44.557 accuracy: 21.934 the time it took: 15.214 seconds\n",
            "[1,  8500] loss: 43.752 accuracy: 22.934 the time it took: 15.238 seconds\n",
            "[1,  9000] loss: 43.457 accuracy: 23.431 the time it took: 16.214 seconds\n",
            "[1,  9500] loss: 43.162 accuracy: 23.550 the time it took: 15.202 seconds\n",
            "[1, 10000] loss: 42.905 accuracy: 23.972 the time it took: 15.358 seconds\n",
            "[1, 10500] loss: 43.511 accuracy: 23.503 the time it took: 15.196 seconds\n",
            "[1, 11000] loss: 43.765 accuracy: 21.822 the time it took: 15.268 seconds\n",
            "[1, 11500] loss: 43.160 accuracy: 22.778 the time it took: 15.799 seconds\n",
            "[1, 12000] loss: 43.055 accuracy: 23.728 the time it took: 15.740 seconds\n",
            "[1, 12500] loss: 42.836 accuracy: 23.884 the time it took: 14.964 seconds\n",
            "Epoch 1 took 389.4621856212616 seconds\n",
            "Accuracy on test set is: 49.93\n",
            "Saved model checkpoint!\n",
            "[2,   500] loss: 42.859 accuracy: 23.816 the time it took: 16.934 seconds\n",
            "[2,  1000] loss: 42.338 accuracy: 24.725 the time it took: 15.879 seconds\n",
            "[2,  1500] loss: 41.890 accuracy: 25.525 the time it took: 15.200 seconds\n",
            "[2,  2000] loss: 41.464 accuracy: 26.334 the time it took: 15.453 seconds\n",
            "[2,  2500] loss: 41.458 accuracy: 26.209 the time it took: 15.359 seconds\n",
            "[2,  3000] loss: 41.024 accuracy: 26.909 the time it took: 15.345 seconds\n",
            "[2,  3500] loss: 41.023 accuracy: 26.556 the time it took: 16.420 seconds\n",
            "[2,  4000] loss: 40.861 accuracy: 26.262 the time it took: 15.392 seconds\n",
            "[2,  4500] loss: 40.470 accuracy: 27.969 the time it took: 15.351 seconds\n",
            "[2,  5000] loss: 40.145 accuracy: 28.094 the time it took: 15.423 seconds\n",
            "[2,  5500] loss: 39.967 accuracy: 27.872 the time it took: 15.304 seconds\n",
            "[2,  6000] loss: 39.992 accuracy: 28.050 the time it took: 16.209 seconds\n",
            "[2,  6500] loss: 39.842 accuracy: 28.491 the time it took: 15.465 seconds\n",
            "[2,  7000] loss: 40.259 accuracy: 27.109 the time it took: 15.475 seconds\n",
            "[2,  7500] loss: 40.113 accuracy: 27.122 the time it took: 15.401 seconds\n",
            "[2,  8000] loss: 39.728 accuracy: 28.828 the time it took: 15.421 seconds\n",
            "[2,  8500] loss: 39.323 accuracy: 29.059 the time it took: 16.010 seconds\n",
            "[2,  9000] loss: 39.030 accuracy: 29.538 the time it took: 15.950 seconds\n",
            "[2,  9500] loss: 38.646 accuracy: 30.044 the time it took: 15.459 seconds\n",
            "[2, 10000] loss: 38.510 accuracy: 30.159 the time it took: 15.571 seconds\n",
            "[2, 10500] loss: 39.273 accuracy: 29.316 the time it took: 15.355 seconds\n",
            "[2, 11000] loss: 39.505 accuracy: 27.397 the time it took: 16.269 seconds\n",
            "[2, 11500] loss: 39.014 accuracy: 28.309 the time it took: 15.579 seconds\n",
            "[2, 12000] loss: 38.863 accuracy: 29.506 the time it took: 15.431 seconds\n",
            "[2, 12500] loss: 38.827 accuracy: 29.447 the time it took: 15.034 seconds\n",
            "Epoch 2 took 391.02378153800964 seconds\n",
            "Accuracy on test set is: 66.22\n",
            "Saved model checkpoint!\n",
            "[3,   500] loss: 38.248 accuracy: 29.725 the time it took: 17.286 seconds\n",
            "[3,  1000] loss: 38.330 accuracy: 30.291 the time it took: 15.665 seconds\n",
            "[3,  1500] loss: 37.905 accuracy: 31.525 the time it took: 15.323 seconds\n",
            "[3,  2000] loss: 37.718 accuracy: 32.044 the time it took: 15.327 seconds\n",
            "[3,  2500] loss: 37.731 accuracy: 31.334 the time it took: 16.011 seconds\n",
            "[3,  3000] loss: 37.358 accuracy: 32.097 the time it took: 15.715 seconds\n",
            "[3,  3500] loss: 37.419 accuracy: 31.519 the time it took: 15.368 seconds\n",
            "[3,  4000] loss: 37.107 accuracy: 31.650 the time it took: 15.385 seconds\n",
            "[3,  4500] loss: 37.006 accuracy: 33.053 the time it took: 15.380 seconds\n",
            "[3,  5000] loss: 36.778 accuracy: 33.353 the time it took: 15.734 seconds\n",
            "[3,  5500] loss: 36.622 accuracy: 32.962 the time it took: 15.989 seconds\n",
            "[3,  6000] loss: 36.616 accuracy: 32.888 the time it took: 15.481 seconds\n",
            "[3,  6500] loss: 36.476 accuracy: 33.594 the time it took: 15.391 seconds\n",
            "[3,  7000] loss: 36.772 accuracy: 32.528 the time it took: 15.350 seconds\n",
            "[3,  7500] loss: 36.507 accuracy: 32.519 the time it took: 15.517 seconds\n",
            "[3,  8000] loss: 36.180 accuracy: 34.394 the time it took: 16.252 seconds\n",
            "[3,  8500] loss: 35.693 accuracy: 35.228 the time it took: 15.393 seconds\n",
            "[3,  9000] loss: 35.583 accuracy: 34.931 the time it took: 15.463 seconds\n",
            "[3,  9500] loss: 35.531 accuracy: 35.328 the time it took: 15.268 seconds\n",
            "[3, 10000] loss: 35.593 accuracy: 34.844 the time it took: 15.646 seconds\n",
            "[3, 10500] loss: 36.466 accuracy: 33.513 the time it took: 16.330 seconds\n",
            "[3, 11000] loss: 36.688 accuracy: 32.003 the time it took: 15.564 seconds\n",
            "[3, 11500] loss: 36.294 accuracy: 32.700 the time it took: 15.319 seconds\n",
            "[3, 12000] loss: 36.223 accuracy: 33.228 the time it took: 15.413 seconds\n"
          ]
        }
      ],
      "source": [
        "result = main_algorithm(alex, AlexnetSurrogate, criterion, generate_random_data_cifar_old, num_of_classes,\n",
        "                        k, epsilon, population_size, generations, search_spread, epochs,\n",
        "                        optimizer_name=optimizer_name, small_dataset=False)\n",
        "\n",
        "test_acc = alex.test_model()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ibeR1CcjViYZ",
        "ArFAzt6LV1C5",
        "nLb4mOnpV1LY"
      ],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}